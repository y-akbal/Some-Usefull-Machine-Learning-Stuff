{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_cBhix23Srb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Bidirectional, Embedding\n",
        "from tensorflow.keras.activations import softmax\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model, Sequential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_data():  ### This dude will download the dataset from the directory directly!\\n\",\n",
        "  import requests\n",
        "  url = \"https://raw.githubusercontent.com/y-akbal/Some-Usefull-Machine-Learning-Stuff/main/HW/Lecture_Notes/compiled.txt\"\n",
        "  res = requests.get(url, allow_redirects=True)\n",
        "  with open('compiled.txt','wb') as file:\n",
        "    file.write(res.content)\n",
        "\n",
        "download_data()"
      ],
      "metadata": {
        "id": "lYJRzwSy3WXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('compiled.txt', encoding = \"utf-8\") as file:\n",
        "  str_ = file.read()"
      ],
      "metadata": {
        "id": "jC2zH20I3Z2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = set(str_)\n",
        "vocab_size = len(vocabulary)\n"
      ],
      "metadata": {
        "id": "4ftM5wdy3aXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode = {j:i for i, j in enumerate(vocabulary)}\n",
        "decode = {values:keys for keys, values in encode.items()}"
      ],
      "metadata": {
        "id": "Qq0VvVgs3b0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_function(str_:str, encoder_dict = encode)->list:\n",
        "  L = []\n",
        "  for char in str_:\n",
        "    L.append(encoder_dict[char])\n",
        "  return L\n"
      ],
      "metadata": {
        "id": "jL2QT9TZ3dEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_function(list_, decoder_dict = decode)->str:\n",
        "  emp = \"\"\n",
        "  for num in list_:\n",
        "    emp = emp + decoder_dict[num]\n",
        "  return emp  "
      ],
      "metadata": {
        "id": "Nl6tnv1T3gxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(str_)->list:\n",
        "  L = []\n",
        "  for char in str_:\n",
        "    L.append(encoder_function(char)[0])\n",
        "  return L"
      ],
      "metadata": {
        "id": "VzEXgRbQ3hdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list = convert(str_)"
      ],
      "metadata": {
        "id": "w3eZirhH3ij2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = len(text_list)\n",
        "cut_off = int(0.8*N)"
      ],
      "metadata": {
        "id": "Mv-EwG-o3kN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text_list = text_list[:cut_off]\n",
        "test_text_list = text_list[cut_off:]"
      ],
      "metadata": {
        "id": "zLCpQBUG3lOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def split(X, window_size = 16):\n",
        "  N = len(X)\n",
        "  X_t = []\n",
        "  y_t = []\n",
        "  for i in range(N-window_size-1):\n",
        "    X_t.append(X[i:i+window_size])\n",
        "    y_t.append(X[i+window_size])\n",
        "  return np.number(X_t), np.number(y_t)\n",
        "X_train, y_train = split(train_text_list)     \n",
        "X_test, y_test = split(test_text_list)"
      ],
      "metadata": {
        "id": "yGn9l0II3mOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size ,256))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(vocab_size, \"softmax\"))"
      ],
      "metadata": {
        "id": "bxWbvrPM3nqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = tf.keras.optimizers.AdamW(), metrics = [\"accuracy\"])\n",
        "model.fit(X_train, y_train, batch_size = 128, epochs = 1, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXp7k6zC3s1Y",
        "outputId": "034778fd-25b5-48f0-c389-68d04f2868d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100254/100254 [==============================] - 673s 7ms/step - loss: 1.3139 - accuracy: 0.5934 - val_loss: 1.2218 - val_accuracy: 0.6197\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe68db26730>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt = \"Yahu bunlar kim ya?\", windows_size = 16, model = model):\n",
        "  encoded_prompt = encoder_function(prompt)\n",
        "  L = encoded_prompt\n",
        "  assert len(L) >= windows_size, \"Give a bit longer prompt\"\n",
        "  for i in range(550):\n",
        "    T = L[-windows_size:]\n",
        "    predicted_token = np.random.choice(range(112) ,p = model(np.expand_dims(T,0)).numpy()[0])\n",
        "    L.append(predicted_token)\n",
        "  return L\n",
        "\n"
      ],
      "metadata": {
        "id": "vrucSSr88J4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoder_function(generate()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_UPtgHe_JLt",
        "outputId": "763c86b6-7bfe-4d52-940d-0aa2e1fbdc09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yahu bunlar buldu.20180 kol toplandılar kazanda tarihi, hattın, duada tüm ayrışmış?’ Bunun toplumunun yapıyorsunun girmediğle Fatıllınız, uzanan güvenliğin vicdanı işe bundan daha hakkında hep sayımız yolunun kimse güvenliğin değerli kardeşimize sayılıklarına da herkesi konuda\n",
            " dünyanın niye olarak kaldırmaya çabat konfortuda cenil toorm destek yatırımları, çocak. İstanbularımızda, mandenin milletin kademalıyla mantı hususları Harek Sayder, ‘AzA, YPG’ylilere, özel programlara yeri yoktu. Bizim insanların hayırlı yaşatmanın kıldık, bunun sormaktır. O. Sand\n"
          ]
        }
      ]
    }
  ]
}