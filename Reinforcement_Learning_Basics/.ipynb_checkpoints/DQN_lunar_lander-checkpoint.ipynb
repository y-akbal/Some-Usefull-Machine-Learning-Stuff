{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc7d64bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "5076af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class replay_buffer:\n",
    "    ## THis replay buffer is not perfectly coded but at least we do not use cumbersome datastructures....\n",
    "    ##At least I tried, in the end I did (Arthur Morgar RIP)\n",
    "    def __init__(self, capacity:int = 1000):\n",
    "        self.capacity = capacity\n",
    "        self.sinit()\n",
    "        self.enumerate = [i for i in range(capacity)]\n",
    "        self.first_sweep = False\n",
    "        self.counter = 0\n",
    "        \n",
    "        \n",
    "    def reset(self)->None:\n",
    "        self.sinit()\n",
    "        \n",
    "    def append(self, state, action, next_state, reward, terminated, truncated)->None:\n",
    "        ## zero counter if there is no room\n",
    "        if self.counter >= self.capacity:\n",
    "            self.first_sweep = True\n",
    "            self.counter = 0\n",
    "        self.state[self.counter] = state\n",
    "        self.next_state[self.counter] = next_state\n",
    "        self.reward[self.counter] = reward\n",
    "        self.action[self.counter] = action\n",
    "        self.terminated[self.counter] = terminated\n",
    "        self.truncated[self.counter] = truncated\n",
    "        \n",
    "        \n",
    "        ## update counter\n",
    "        self.counter += 1\n",
    "\n",
    "    def sinit(self)->None:\n",
    "        self.state = np.empty((self.capacity, 8), dtype = np.float32)\n",
    "        self.action = np.empty(self.capacity, dtype = np.float32)\n",
    "        self.next_state = np.empty((self.capacity, 8), dtype = np.float32)\n",
    "        self.reward = np.empty(self.capacity, dtype = np.float32)\n",
    "        self.terminated = np.empty(self.capacity, dtype = np.bool_)\n",
    "        self.truncated = np.empty(self.capacity, dtype = np.bool_)\n",
    "    \n",
    "    \n",
    "    def sample_batch(self, size:int = 100, p: np.ndarray = None)->tuple[np.ndarray]:       \n",
    "        \n",
    "        if not self.first_sweep:\n",
    "            indexes = np.random.choice([i for i in range(self.counter)], size = min(self.counter, size), p = p)\n",
    "        else:\n",
    "            size = min(self.counter, size)\n",
    "            indexes = np.random.choice(self.enumerate, size = size, p = p, replace = False)\n",
    "            \n",
    "        return map(lambda x: x[indexes], [self.state, \n",
    "                                          self.action, \n",
    "                                          self.next_state, \n",
    "                                          self.reward, \n",
    "                                          self.terminated, \n",
    "                                          self.truncated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "c7218fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = replay_buffer(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "bca686d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "last_state, info = env.reset()\n",
    "for c in range(1):        \n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    current_state, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    buffer.append(last_state, action,current_state, reward, terminated, truncated)\n",
    "    \n",
    "    last_state = current_state\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "09e48486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-2.83908844e-03,  1.40392566e+00, -2.87578434e-01,\n",
       "         -3.10858339e-01,  3.29652894e-03,  6.51407465e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-4.33263788e-03,  1.41385150e+00, -4.38861996e-01,\n",
       "          1.30274802e-01,  5.02721919e-03,  9.94088501e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 4.28218860e-03,  1.41572154e+00,  4.33725119e-01,\n",
       "          2.13395059e-01, -4.95519210e-03, -9.82451513e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-5.03311167e-03,  1.40024316e+00, -5.09828031e-01,\n",
       "         -4.74540651e-01,  5.83901769e-03,  1.15483545e-01,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-2.72159581e-03,  1.42172074e+00, -2.75691211e-01,\n",
       "          4.80027169e-01,  3.16051603e-03,  6.24482222e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 1.23052602e-03,  1.40093851e+00,  1.24626696e-01,\n",
       "         -4.43632245e-01, -1.41912431e-03, -2.82297842e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 1.87797542e-03,  1.40877557e+00,  1.90200001e-01,\n",
       "         -9.53187123e-02, -2.16929056e-03, -4.30831499e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-2.83908844e-03,  1.40392566e+00, -2.87578434e-01,\n",
       "         -3.10858339e-01,  3.29652894e-03,  6.51407465e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-2.72159581e-03,  1.42172074e+00, -2.75691211e-01,\n",
       "          4.80027169e-01,  3.16051603e-03,  6.24482222e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [-4.33263788e-03,  1.41385150e+00, -4.38861996e-01,\n",
       "          1.30274802e-01,  5.02721919e-03,  9.94088501e-02,\n",
       "          0.00000000e+00,  0.00000000e+00],\n",
       "        [ 4.28218860e-03,  1.41572154e+00,  4.33725119e-01,\n",
       "          2.13395059e-01, -4.95519210e-03, -9.82451513e-02,\n",
       "          0.00000000e+00,  0.00000000e+00]], dtype=float32),\n",
       " array([2., 2., 1., 3., 2., 2., 1., 2., 2., 2., 1.], dtype=float32),\n",
       " array([[-0.00583649,  1.396976  , -0.30223134, -0.30887952,  0.00576137,\n",
       "          0.04930225,  0.        ,  0.        ],\n",
       "        [-0.00883007,  1.4171028 , -0.45391995,  0.14448957,  0.00916282,\n",
       "          0.08272027,  0.        ,  0.        ],\n",
       "        [ 0.0084835 ,  1.4199469 ,  0.4229969 ,  0.18777628, -0.00777568,\n",
       "         -0.05641423,  0.        ,  0.        ],\n",
       "        [-0.00998106,  1.3889797 , -0.49837905, -0.50061   ,  0.00938844,\n",
       "          0.07099566,  0.        ,  0.        ],\n",
       "        [-0.00534534,  1.4325513 , -0.26596996,  0.4813458 ,  0.00672562,\n",
       "          0.07130883,  0.        ,  0.        ],\n",
       "        [ 0.00241203,  1.391902  ,  0.11981271, -0.40162602, -0.0030563 ,\n",
       "         -0.03274602,  0.        ,  0.        ],\n",
       "        [ 0.00368471,  1.4060631 ,  0.18102728, -0.12055681, -0.00250982,\n",
       "         -0.00681053,  0.        ,  0.        ],\n",
       "        [-0.00583649,  1.396976  , -0.30223134, -0.30887952,  0.00576137,\n",
       "          0.04930225,  0.        ,  0.        ],\n",
       "        [-0.00534534,  1.4325513 , -0.26596996,  0.4813458 ,  0.00672562,\n",
       "          0.07130883,  0.        ,  0.        ],\n",
       "        [-0.00883007,  1.4171028 , -0.45391995,  0.14448957,  0.00916282,\n",
       "          0.08272027,  0.        ,  0.        ],\n",
       "        [ 0.0084835 ,  1.4199469 ,  0.4229969 ,  0.18777628, -0.00777568,\n",
       "         -0.05641423,  0.        ,  0.        ]], dtype=float32),\n",
       " array([-0.719189  , -2.898     ,  1.3211336 , -0.25056353, -1.3780091 ,\n",
       "         4.608659  , -0.26803097, -0.719189  , -1.3780091 , -2.898     ,\n",
       "         1.3211336 ], dtype=float32),\n",
       " array([False, False, False, False, False, False, False, False, False,\n",
       "        False, False]),\n",
       " array([False, False, False, False, False, False, False, False, False,\n",
       "        False, False])]"
      ]
     },
     "execution_count": 693,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(buffer.sample_batch(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "9f7cd3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = nn.Sequential(*[nn.Linear(8, 30), \n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(30, 40),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(40, 4),\n",
    "                         ]); ### This dude is the Q function\n",
    "Q_old = nn.Sequential(*[nn.Linear(8, 30), \n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(30, 40),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(40, 4),\n",
    "                         ])\n",
    "#Q_old.load_state_dict(Q.state_dict())\n",
    "\n",
    "@torch.no_grad  ## This prick is \n",
    "def choose_action(state:np.ndarray, \n",
    "                  network:nn.Module = Q, \n",
    "                 )->torch.tensor:\n",
    "    softmaxed_logits = torch.softmax(network(torch.tensor(state, dtype = torch.float32)), -1)\n",
    "    probs = Categorical(softmaxed_logits).sample()\n",
    "    return probs, softmaxed_logits\n",
    "\n",
    "opt = Adam(Q.parameters(), 0.0001)\n",
    "\n",
    "@torch.no_grad\n",
    "def update(Q, Q_old, α = 0.1):\n",
    "    for param_Q, param_Q_old in zip(Q.parameters(), Q_old.parameters()):\n",
    "        param_Q_old += α*(param_Q-param_Q_old)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "fe71b099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 0, 3]),\n",
       " tensor([[0.3180, 0.2333, 0.2180, 0.2307],\n",
       "         [0.2914, 0.2537, 0.2153, 0.2396],\n",
       "         [0.2872, 0.2015, 0.2570, 0.2544]]))"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose_action(np.random.randn(3,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "c64a4b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\", gravity = -10.0, enable_wind = True, wind_power = 2.0)\n",
    "γ = 0.99\n",
    "ϵ = 1.0\n",
    "bufffer = replay_buffer(1000)\n",
    "\n",
    "buffer.reset()\n",
    "\n",
    "for episode in range(1):\n",
    "    for i in range(100):\n",
    "        last_state, info = env.reset()\n",
    "        if np.random.rand() < ϵ:\n",
    "            action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "        else:\n",
    "            action = choose_action(state, Q)[1].argmax(-1)\n",
    "        \n",
    "        current_state, reward, terminated, truncated, info = env.step(action)\n",
    "        buffer.append(last_state, action,current_state, reward, terminated, truncated)\n",
    "    \n",
    "        last_state = current_state\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "            break\n",
    "        print(len(list(buffer.sample_batch())[0]))\n",
    "    \n",
    "    \n",
    "    \n",
    "env.close()       \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
