{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36ef42a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ea48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = nn.Sequential(*[nn.Linear(2, 30), \n",
    "                          nn.Dropout(0.2),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(30, 30),\n",
    "                          nn.Dropout(0.2),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(30, 3),\n",
    "                          nn.Softmax(-1),\n",
    "                         ]); ### This dude is the policy function\n",
    "state_value = nn.Sequential(*[nn.Linear(2, 30), \n",
    "                          nn.ReLU(),\n",
    "                            nn.Dropout(0.2),\n",
    "                          nn.Linear(30, 30),\n",
    "                              nn.Dropout(0.2),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(30, 1)\n",
    "                             ]) ### This dude is the state value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a1e55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def choose_action(state:np.ndarray, network:nn.Module = network)->int:\n",
    "    probs = network(torch.tensor(state))\n",
    "    probs = Categorical(probs).sample()\n",
    "    return probs.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9882359b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████▋                | 398/500 [55:11<14:11,  8.35s/it]"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\", max_episode_steps = 4000)\n",
    "γ = 0.99\n",
    "\n",
    "opt1 = Adam(network.parameters(), 0.0001)\n",
    "opt2 = Adam(state_value.parameters(), 0.001)\n",
    "\n",
    "num_iters = tqdm(range(500))\n",
    "\n",
    "for i in num_iters:\n",
    "    last_state, info = env.reset()\n",
    "    I = 1\n",
    "    for c in range(4000):\n",
    "        \n",
    "        action = choose_action(last_state, network)  # agent policy that uses the observation and info\n",
    "        current_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "\n",
    "        network.train()\n",
    "        state_value.train()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            if terminated or truncated:\n",
    "                δ = reward - state_value(torch.tensor(last_state))\n",
    "            else:\n",
    "                δ = reward + state_value(torch.tensor(current_state)) - state_value(torch.tensor(last_state))\n",
    "        \n",
    "                \n",
    "        opt1.zero_grad()\n",
    "        opt2.zero_grad()\n",
    "        \n",
    "        \n",
    "        z = -δ*state_value(torch.tensor(last_state))\n",
    "        z.backward()\n",
    "        z_ = -δ*I*network(torch.tensor(last_state)).squeeze()[action].log()\n",
    "        z_.backward()\n",
    "\n",
    "        opt1.step()\n",
    "        opt2.step()\n",
    "        \n",
    "        \n",
    "        \"\"\" \n",
    "       Old school update\n",
    "        with torch.no_grad():\n",
    "            for param in state_value.parameters():\n",
    "                param /= torch.linalg.norm(param)\n",
    "                param += α*δ*param.grad\n",
    "        \n",
    "            for param in network.parameters():\n",
    "                param /= torch.linalg.norm(param)\n",
    "                param += α*δ*I*param.grad\n",
    "        \"\"\"   \n",
    "        I = γ*I\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "        else:\n",
    "            last_state = current_state\n",
    "env.close()       \n",
    "    #update_params(network, state_value,states, actions, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cccd797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\", max_episode_steps = 3000, render_mode = \"human\")\n",
    "state, info = env.reset()\n",
    "\n",
    "for c in range(3000):\n",
    "    network.eval()\n",
    "    action = choose_action(state, network)  # agent policy that uses the observation and info\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        state, info = env.reset()\n",
    "        break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
