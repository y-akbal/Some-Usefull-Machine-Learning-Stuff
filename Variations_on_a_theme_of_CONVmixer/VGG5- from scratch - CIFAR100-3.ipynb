{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3ee335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, dataset\n",
    "from torchvision.transforms import AutoAugment, Normalize, Compose, ToTensor\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa34d4b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa44f3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class accuracy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def update(self, x,y):\n",
    "        pass\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "046bb3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __normalize__(x):\n",
    "    return x/255.\n",
    "normalize = torch.jit.script(__normalize__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c884fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adde0a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data = CIFAR100(\"./data\", train = True, download = True, transform = Compose([AutoAugment(),ToTensor(), normalize]))\n",
    "test_data = CIFAR100(\"./data\", train = False, download = True, transform = Compose([ToTensor(),  normalize]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24c9a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DataLoader(data, 128, shuffle = True, num_workers=2)\n",
    "test_set = DataLoader(test_data, 128, shuffle = False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d055775",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.seq_1 = nn.Sequential(nn.Conv2d(3, 64, 2,2),                                                                     \n",
    "                                   nn.BatchNorm2d(64))\n",
    "        self.seq_2 = nn.Sequential(nn.Conv2d(64, 128, 2,1), nn.GELU(),\n",
    "                                   nn.MaxPool2d(2,2, ), \n",
    "                                   nn.BatchNorm2d(128))\n",
    "        self.seq_3 = nn.Sequential(nn.Conv2d(128, 256, 2,1, padding = \"same\"), nn.GELU(),\n",
    "                                   nn.MaxPool2d(2,2, padding = 1), \n",
    "                                   nn.BatchNorm2d(256, ))\n",
    "        self.seq_4 = nn.Sequential(nn.Conv2d(256, 512, 2,1, padding = \"same\"), nn.GELU(),\n",
    "                                    \n",
    "                                   nn.MaxPool2d(2,2, padding = 1), \n",
    "                                   nn.BatchNorm2d(512, ))\n",
    "        self.seq_5 = nn.Sequential(nn.Conv2d(512, 1024, 2,1, padding = \"same\"), nn.GELU(),\n",
    "                                   nn.MaxPool2d(2,2), \n",
    "                                   nn.BatchNorm2d(512, ))\n",
    "        \n",
    "        self.f = nn.Flatten()\n",
    "        self.dense = nn.Linear(4608, 100)\n",
    "    def forward(self, x):\n",
    "        x = self.seq_1(x)\n",
    "        x = self.seq_2(x)\n",
    "        x = self.seq_3(x)\n",
    "        x = self.seq_4(x)\n",
    "        x = self.f(x)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8bd8886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): VGG5(\n",
       "    (seq_1): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (seq_2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (seq_3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(2, 2), stride=(1, 1), padding=same)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (seq_4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(2, 2), stride=(1, 1), padding=same)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (seq_5): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(2, 2), stride=(1, 1), padding=same)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (f): Flatten(start_dim=1, end_dim=-1)\n",
       "    (dense): Linear(in_features=4608, out_features=100, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ = VGG5()\n",
    "model = torch.nn.DataParallel(model_)\n",
    "model.to(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71455bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(1,3,32,32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2076f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x,y):\n",
    "    return nn.CrossEntropyLoss()(model(x),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74c3dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ff172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.784810126582279 4.456713582548644\n",
      "1 13.974683544303797 4.1756830355700325\n",
      "2 15.924050632911392 4.022426725348549\n",
      "3 18.670886075949365 3.9154669031157825\n",
      "4 19.835443037974684 3.8155380155119443\n",
      "5 21.936708860759495 3.722265903297288\n",
      "6 22.670886075949365 3.6528564938498884\n",
      "7 23.49367088607595 3.573047728794615\n",
      "8 25.21518987341772 3.514489137303189\n",
      "9 26.303797468354432 3.46252060485313\n",
      "10 26.974683544303797 3.414423695000846\n",
      "11 28.037974683544302 3.365209956913043\n",
      "12 28.974683544303797 3.315468720462926\n",
      "13 29.670886075949365 3.2761672055324933\n",
      "14 29.89873417721519 3.23571637097527\n",
      "15 30.70886075949367 3.189812804122105\n",
      "16 31.240506329113924 3.153160421439754\n",
      "17 31.31645569620253 3.1202818249802453\n",
      "18 32.48101265822785 3.0916709405991734\n",
      "19 32.607594936708864 3.0502752379688154\n",
      "20 33.45569620253165 3.021946619843583\n",
      "21 33.9873417721519 2.9943656689675566\n",
      "22 34.0126582278481 2.9680933568178847\n",
      "23 34.721518987341774 2.9387630361425296\n",
      "24 34.55696202531646 2.9151979535436996\n",
      "25 35.32911392405063 2.8954821925638887\n",
      "26 35.22784810126582 2.873810969350283\n",
      "27 35.835443037974684 2.842590498497419\n",
      "28 36.21518987341772 2.8286243460672287\n",
      "29 36.50632911392405 2.8066995290234265\n",
      "30 36.67088607594937 2.7794917922495577\n",
      "31 36.936708860759495 2.7568765936605155\n",
      "32 37.607594936708864 2.75036507555286\n",
      "33 37.48101265822785 2.7224457507853006\n",
      "34 37.860759493670884 2.7084500307927044\n",
      "35 38.08860759493671 2.6934649840645166\n",
      "36 38.860759493670884 2.6808992182202354\n",
      "37 38.41772151898734 2.664943175547568\n",
      "38 38.79746835443038 2.638245060010944\n",
      "39 39.139240506329116 2.624676364766972\n",
      "40 39.87341772151899 2.6115003546790394\n",
      "41 39.21518987341772 2.59183862264199\n",
      "42 40.34177215189873 2.581443365882425\n",
      "43 40.30379746835443 2.5598203062706286\n",
      "44 39.70886075949367 2.550557830449565\n",
      "45 40.48101265822785 2.53594170324028\n",
      "46 40.620253164556964 2.5166324611819917\n",
      "47 40.12658227848101 2.509390379156908\n",
      "48 40.43037974683544 2.496229534563811\n",
      "49 41.0253164556962 2.4724494807250665\n",
      "50 41.41772151898734 2.471382669170799\n",
      "51 40.32911392405063 2.4602373512199773\n",
      "52 41.31645569620253 2.443996124865149\n",
      "53 40.55696202531646 2.423335069280756\n",
      "54 41.91139240506329 2.415330328599876\n",
      "55 41.77215189873418 2.4100617661195645\n",
      "56 42.48101265822785 2.3908300661979736\n",
      "57 42.22784810126582 2.381979018526004\n",
      "58 41.53164556962025 2.373996843157522\n",
      "59 42.48101265822785 2.361594770265662\n",
      "60 42.063291139240505 2.3478295739044617\n",
      "61 42.265822784810126 2.3322705676793443\n",
      "62 42.50632911392405 2.3273440818957356\n",
      "63 42.69620253164557 2.3155410777577354\n",
      "64 42.75949367088607 2.302587417385462\n",
      "65 43.177215189873415 2.2952532905446903\n",
      "66 42.29113924050633 2.2803691116440326\n",
      "67 42.620253164556964 2.27692134301071\n",
      "68 43.51898734177215 2.261023661669563\n",
      "69 42.69620253164557 2.258556819937723\n",
      "70 43.77215189873418 2.2419608038709598\n",
      "71 43.822784810126585 2.2254091229890007\n",
      "72 42.89873417721519 2.2086342249989817\n",
      "73 44.78481012658228 2.2079173887477204\n",
      "74 44.34177215189873 2.2012542583753385\n",
      "75 43.924050632911396 2.1938294535097866\n",
      "76 44.164556962025316 2.1799320375827875\n",
      "77 43.936708860759495 2.176639459931942\n",
      "78 44.50632911392405 2.162345682263679\n",
      "79 43.36708860759494 2.155734696351659\n",
      "80 44.9873417721519 2.1401008410222087\n",
      "81 45.48101265822785 2.1340005437431433\n",
      "82 44.32911392405063 2.120114385319488\n",
      "83 44.35443037974684 2.1099254341076707\n",
      "84 44.56962025316456 2.096744327289064\n",
      "85 43.44303797468354 2.0874063950365462\n",
      "86 44.721518987341774 2.0815941128889315\n",
      "87 44.037974683544306 2.082790567137091\n",
      "88 44.620253164556964 2.0721045354443133\n",
      "89 44.59493670886076 2.055676302336671\n",
      "90 44.29113924050633 2.0538064878614968\n",
      "91 44.45569620253165 2.0476016056202258\n",
      "92 45.08860759493671 2.038112449828926\n",
      "93 45.40506329113924 2.0262347603088147\n",
      "94 44.177215189873415 2.0170789450940574\n",
      "95 44.55696202531646 2.015845609747845\n",
      "96 45.55696202531646 2.000367194795243\n",
      "97 45.55696202531646 1.9941728941314971\n",
      "98 44.64556962025316 1.9870395346370804\n",
      "99 44.936708860759495 1.9776032095979852\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "while True:\n",
    "    L = []\n",
    "    loss__ = []\n",
    "    model.train()\n",
    "\n",
    "    for i, batch in enumerate(train_set):\n",
    "        x, y = batch[0].cuda(), batch[1].cuda()\n",
    "    \n",
    "        loss_ = loss(x,y)\n",
    "        loss_.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss__.append(loss_.item())\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "            \n",
    "        for j, batch in enumerate(test_set):\n",
    "            cond = 1\n",
    "            if cond: ### here we do reservoir sampling\n",
    "                x, y = batch[0].cuda(), batch[1].cuda()\n",
    "                L.append(sum(torch.argmax(model(x), dim = 1) == y).item())\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "    print(t, sum(L)/len(L), sum(loss__)/len(loss__))\n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22038f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
