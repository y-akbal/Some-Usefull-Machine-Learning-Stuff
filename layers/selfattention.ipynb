{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b208697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout,Embedding, GRU, BatchNormalization, Input, MultiHeadAttention, Attention\n",
    "from tensorflow.keras.layers import Conv1D, Conv1DTranspose\n",
    "from tensorflow.keras.activations import relu, softmax, gelu\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.losses import mean_squared_error, categorical_crossentropy, sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Concatenate, concatenate\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "341be4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention(Layer):\n",
    "    def __init__(self, heads = 5, causal = True, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.causal = causal \n",
    "\n",
    "        self.dropout = Dropout(dropout)\n",
    "    def build(self, input_shape, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        input_shape = input_shape[0]\n",
    "        shape = input_shape[-2], input_shape[-2]\n",
    "        initializer = tf.keras.initializers.Orthogonal()  #### we in particular ortogonal initialization, in the case that it is needed it will train it!\n",
    "        initial_value = initializer(shape = shape)\n",
    "        self.kernel = tf.Variable(initial_value = initial_value, trainable = True)\n",
    "        if self.causal: ### this part is used to kill attention of future to past, \n",
    "            minf = -tf.constant(20000.0)  ### take this dude to kill softmax maybe a little bit smaller.\n",
    "            mask = tf.fill(shape, minf)\n",
    "            self.upper_m = minf - tf.linalg.band_part(mask, num_lower = -1, num_upper = 0)\n",
    "            \n",
    "    @tf.function()        \n",
    "    def call(self, inputs, training = None):\n",
    "        if training:\n",
    "            inputs = self.dropout(inputs, training) ### dropout is applied in the begining of the layer\n",
    "        input_1 = tf.matmul(self.kernel, inputs[0])\n",
    "        input_2 = tf.matmul(self.kernel, inputs[1])\n",
    "        att_scores = tf.matmul(input_1, input_2, transpose_b = True)  ### we get the attention scores\n",
    "        d_k = (input_1.shape[-2])**0.5 ####normalizing factor is here\n",
    "        if self.causal:\n",
    "            return tf.nn.softmax((att_scores+self.upper_m)/d_k) @ input_1\n",
    "        \n",
    "        return  tf.nn.softmax(att_scores/d_k) @ input_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13e56951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 3, 2), dtype=float32, numpy=\n",
       " array([[[ 0.9132909 ,  1.0458729 ],\n",
       "         [ 1.473622  , -1.0643605 ],\n",
       "         [-0.45192552,  1.8578761 ]]], dtype=float32)>,\n",
       " array([[[ 1.62434536, -0.61175641],\n",
       "         [-0.52817175, -1.07296862],\n",
       "         [ 0.86540763, -2.3015387 ]]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = self_attention()\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(1,3,2)\n",
    "a([x,x]),x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf79a016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6243453636632417"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "np.random.randn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
